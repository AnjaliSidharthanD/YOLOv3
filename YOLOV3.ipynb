{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOV3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLvhaNFJtcg/UuwtYZv4Ey",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnjaliSidharthanD/YOLOv3/blob/main/YOLOV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwqyUQj_aLl9"
      },
      "source": [
        "#**Introduction to Object detection using YOLO v3**\n",
        "\n",
        "Author: Anjali\n",
        "\n",
        "Object detection is a computer vision technique that involves detecting the presence, location and type of one or more objects in an image. Yolo which stands for ‘you only look once’ is a real-time object detection algorithm that uses deep convolutional neural network. In this notebook, we will discuss YOLOv3, a variant of the original YOLO model that achieves near state-of-the-art(SOTA) result. It is one of the fastest algorithm compared to R - CNN family.\n",
        "\n",
        ">The official neural net implementation from the ground up in C from the author is available at [Darnet](https://pjreddie.com/darknet/) . It is available on [github](https://github.com/pjreddie/darknet) for people to use.\n",
        "\n",
        ">We have two options to get started with object detection:\n",
        "\n",
        "*  Using the pre-trained model\n",
        "*  Training custom object detector from scratch\n",
        "\n",
        "In this notebook, we will be looking at creating an object detector using the pre-trained model for videos. Let us dive into the code.\n",
        "\n",
        "*Useful intro about [Colab](https://colab.research.google.com/notebooks/welcome.ipynb)*\n",
        "\n",
        "*Useful intro about [OpenCV](https://opencv.org/)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI9KMNA6Swrh"
      },
      "source": [
        "## Setting up our notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u22w3BFiOveA"
      },
      "source": [
        "### Mounting Google Drive locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpdOzkkgsC29",
        "outputId": "c577d5c4-d963-456d-b22d-ac53a5e060e2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMf8OM92aBxv"
      },
      "source": [
        "### Choose the Necessary directory from drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsKA1JtKSm7g",
        "outputId": "e49d66ca-cbc0-4496-c0d1-126b4a76cd38"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmMlX5zmYhEt",
        "outputId": "6dd943f6-9122-4f39-a4bb-03a67b0fc743"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/yoloV3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/yoloV3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4Hy2bKvS9LT"
      },
      "source": [
        "## Python code .py files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL7ryc48XJD6"
      },
      "source": [
        "\n",
        "\n",
        "### Scripting yolo.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fcb3MAOOZKYS",
        "outputId": "cc767750-3462-41f2-ade7-8d1a1c06898b"
      },
      "source": [
        "%%writefile yolo.py\n",
        "##### SCRIPT STARTS HERE #####\n",
        "#!usr/bin/bash python\n",
        "# Importing required packages\n",
        "# python yolo_video.py --input videos/airport.mp4 --output output/airport_output.avi --yolo yolo-coco\n",
        "\n",
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Initialize the parameters\n",
        "confidenceThreshold = 0.5  #Confidence threshold\n",
        "nmsThreshold = 0.4         #Non-maximum suppression threshold\n",
        "inputWidth = 416           #Width of network's input image\n",
        "inputHeight = 416          #Height of network's input image\n",
        "\n",
        "# construct the argument parse and parse the arguments\n",
        "parser = argparse.ArgumentParser(description='Object Detection using YOLO in OPENCV')\n",
        "parser.add_argument('--image', help=\"True/False\", default=False)\n",
        "parser.add_argument('--video', help=\"Path to video file\", default=\"videos/car_on_road.mp4\")\n",
        "parser.add_argument('--verbose', help=\"To print statements\", default=True)\n",
        "args = parser.parse_args()\n",
        "\n",
        "#Load YOLO V3\n",
        "\n",
        "def loadYolo():\n",
        "\n",
        "    # load our YOLO object detector trained on COCO dataset (80 classes)\n",
        "    print(\"[INFO] loading YOLO from disk...\")\n",
        "    # derive the paths to the YOLO weights and model configuration\n",
        "    configPath = '/content/gdrive/MyDrive/yoloV3/yolov3.cfg'\n",
        "    weightsPath = '/content/gdrive/MyDrive/yoloV3/yolov3.weights'\n",
        "    net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
        "    #cv2.dnn.readNetFromDarknet(\"yolo3.weights\",\"yolov3.cfg\")\n",
        "\n",
        "    # load the COCO class labels our YOLO model was trained on\n",
        "    classes=[]\n",
        "    # load the COCO class labels our YOLO model was trained on\n",
        "    classesPath = '/content/gdrive/MyDrive/yoloV3/coco.names'\n",
        "    with open(classesPath, \"r\") as f:\n",
        "      classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    # initialize a list of colors to represent each possible class label\n",
        "    np.random.seed(42)\n",
        "    colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
        "    #np.random.randint(0, 255, size=(len(LABELS), 3),dtype=\"uint8\")\n",
        "    \n",
        "\n",
        "    # and determine only the *output* layer names that we need from YOLO\n",
        "    \n",
        "    # Get the names of all the layers in the network\n",
        "    layersNames = net.getLayerNames()\n",
        "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
        "    outputLayers = [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "    \n",
        "    return net, classes, colors, outputLayers\n",
        "\n",
        "def detectObjects(image,net,outputLayers):\n",
        "    # Create a 4D blob from a frame and then perform a forward\n",
        "    # pass of the YOLO object detector, giving us our bounding boxes\n",
        "    # and associated probabilities\n",
        "    blob = cv2.dnn.blobFromImage(image, scalefactor=0.00392, size=(inputWidth, inputHeight), mean=(0, 0, 0), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    layerOutputs = net.forward(outputLayers)\n",
        "    return blob, layerOutputs\n",
        "\n",
        "def getBoundingbox(layerOutputs, height, width):\n",
        "    # initialize our lists of detected bounding boxes, confidences,\n",
        "    # and class IDs, respectively\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    classIDs = []\n",
        "\n",
        "    # loop over each of the layer outputs\n",
        "    for output in layerOutputs:\n",
        "        #loop over each of the detections\n",
        "        for detection in output:\n",
        "            # extract the class ID and confidence (i.e., probability)\n",
        "            # of the current object detection\n",
        "            scores = detection[5:]\n",
        "            classID = np.argmax(scores)\n",
        "            confidence = scores[classID]\n",
        "\n",
        "            #filter out weak predictions by ensuring the detected\n",
        "            # probability is greater than the minimum probability\n",
        "\n",
        "            if confidence > confidenceThreshold:\n",
        "                # scale the bounding box coordinates back relative to\n",
        "                # the size of the image, keeping in mind that YOLO\n",
        "                # actually returns the center (x, y)-coordinates of\n",
        "                # the bounding box followed by the boxes' width and\n",
        "                # height\n",
        "                box = detection[0:4] * np.array([width, height, width, height])\n",
        "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "                \n",
        "                # use the center (x, y)-coordinates to derive the top\n",
        "                # and and left corner of the bounding box\n",
        "                x = int(centerX - (width / 2))\n",
        "                y = int(centerY - (height / 2))\n",
        "\n",
        "                # update our list of bounding box coordinates,\n",
        "                # confidences, and class IDs\n",
        "                boxes.append([x, y, int(width), int(height)])\n",
        "                confidences.append(float(confidence))\n",
        "                classIDs.append(classID)\n",
        "\n",
        "    return boxes, confidences, classIDs\n",
        "\n",
        "\n",
        "# Draw the predicted bounding boxes\n",
        "\n",
        "#def drawPredictedBB(boxes, confidences, colors, classIDs,classes, image):\n",
        "    # apply non-maxima suppression to suppress weak, overlapping\n",
        "    # bounding boxes with lower confidences\n",
        " #   indices = cv2.dnn.NMSBoxes(boxes, confidences, confidenceThreshold, nmsThreshold)\n",
        " #   font = cv2.FONT_HERSHEY_PLAIN\n",
        "    \n",
        "    # ensure at least one detection exists\n",
        " #   if len(indices) > 0:\n",
        "        #loop over indices we are keeping\n",
        " #       for index in indices.flatten():\n",
        "            # extract the bounding box coordinates\n",
        " #           (x, y) = (boxes[index][0], boxes[index][1])\n",
        " #           (width,height) = (boxes[index][2], boxes[index][3])\n",
        "\n",
        "            # Draw a bounding box\n",
        " #           text = \"{}: {:.4f}\".format(classes[classIDs[index]],confidences[index])\n",
        " #           color = [int(c) for c in colors[classIDs[index]]]\n",
        " #           cv2.rectangle(image, (x,y), (x+width, y+height), color,2)\n",
        " #           cv2.putText(image, text, (x, y - 5), font, 1, color, 1)\n",
        " #\n",
        "def videoToFrames(videoPath): \n",
        "    !rm -r inputFrames/*\n",
        "    !mkdir inputFrames/  \n",
        "    frameCount=0\n",
        "    inputFrames=[]\n",
        "    capture = cv2.VideoCapture(videoPath)\n",
        "    while(True):\n",
        "        # Capture the video frame by frame from the file\n",
        "        (grabbed,frame) = capture.read()\n",
        "        # if the frame was not grabbed, then we have reached the end\n",
        "        #of the stream\n",
        "        if not grabbed:\n",
        "          print(\"[INFO] All frames appended !!!\")\n",
        "          break\n",
        "        inputFrames.append(frame)\n",
        "        frameCount = frameCount + 1\n",
        "        cv2.imwrite('frames/'+str(frameCount)+'.png', frame)\n",
        "    return inputFrames \n",
        "\n",
        "def framesToVideo():\n",
        "  outputFrames = os.listdir('output/')\n",
        "  outputFrames.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
        "  frames=[]\n",
        "  writer = None\n",
        "  (width,height)= (None, None)\n",
        "  for index in range(len(outputFrames)):\n",
        "    #reading each files\n",
        "    image = cv2.imread('output/'+outputFrames[index])\n",
        "    height, width = image.shape[:2]\n",
        "    size = (width,height)\n",
        "    \n",
        "    #inserting the frames into an image array\n",
        "    frames.append(image)\n",
        "  \n",
        "  size = (width,height)\n",
        "  outputFile = videoPath[:-4]+'_yolo_out_py.mp4'\n",
        "  # initialize our video writer\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "  writer = cv2.VideoWriter(outputFile,fourcc, 29, size)\n",
        "\n",
        "  for index in range(len(frames)):\n",
        "    # writing to a image array\n",
        "         writer.write(frames[index])\n",
        "  writer.release()\n",
        "\n",
        "\n",
        "def startVideo(videoPath):\n",
        "    outputFile = \"yolo_out_py.avi\"\n",
        "    net, classes, colors, outputLayers = loadYolo()\n",
        "    # initialize the video stream, pointer to output video file, and\n",
        "    # frame dimensions\n",
        "    capture = cv2.VideoCapture(videoPath)\n",
        "    outputFile = videoPath[:-4]+'_yolo_out_py.avi'\n",
        "    writer = None\n",
        "    (width,height) = (None, None)\n",
        "    i= 0\n",
        "    # loop over frames from the video file stream\n",
        "    while(True):\n",
        "        # Capture the video frame by frame from the file\n",
        "        (grabbed,frame) = capture.read()\n",
        "        # if the frame was not grabbed, then we have reached the end\n",
        "        #of the stream\n",
        "        if not grabbed:\n",
        "          print(\"Done processing !!!\")\n",
        "          print(\"Output file is stored as \", outputFile)\n",
        "          break\n",
        "\n",
        "        # if the frame dimensions are empty, grab them\n",
        "        if height is None or width is None:\n",
        "            height,width = frame.shape[:2]\n",
        "            \n",
        "        # construct a blob from the input frame and then perform a forward\n",
        "\t      # pass of the YOLO object detector, giving us our bounding boxes\n",
        "\t      # and associated probabilities\n",
        "        blob, layerOutputs = detectObjects(frame, net, outputLayers)\n",
        "        boxes, confidences, classIDs = getBoundingbox(layerOutputs, height, width)\n",
        "        # apply non-maxima suppression to suppress weak, overlapping\n",
        "        # bounding boxes with lower confidences\n",
        "        indices = cv2.dnn.NMSBoxes(boxes, confidences, confidenceThreshold, nmsThreshold)\n",
        "        font = cv2.FONT_HERSHEY_PLAIN\n",
        "        # ensure at least one detection exists\n",
        "        if len(indices) > 0:\n",
        "            #loop over indices we are keeping\n",
        "            for index in indices.flatten():\n",
        "                # extract the bounding box coordinates\n",
        "                (x, y) = (boxes[index][0], boxes[index][1])\n",
        "                (width,height) = (boxes[index][2], boxes[index][3])\n",
        "\n",
        "                # Draw a bounding box\n",
        "                text = \"{}: {:.4f}\".format(classes[classIDs[index]],confidences[index])\n",
        "                color = [int(c) for c in colors[classIDs[index]]]\n",
        "                cv2.rectangle(frame, (x,y), (x+width, y+height), color,2)\n",
        "                cv2.putText(frame, text, (x, y - 5), font, 1, color, 1)\n",
        "   \n",
        "        #Check if the vivideo writer is None\n",
        "        if writer is None:\n",
        "            # initialize our video writer\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "            writer = cv2.VideoWriter(outputFile,fourcc, 29, size)\n",
        "\n",
        "        # write the output frame to disk\n",
        "        writer.write(frame.astype(np.uint8))\n",
        "        print(\"[INFO] frame\",i)\n",
        "        i +=1\n",
        "        if cv2.waitKey(1) & 0xFF == ord('s'):\n",
        "            break\n",
        "    # release the file pointers\n",
        "    print(\"[INFO] cleaning up...\")\n",
        "    writer.release()\n",
        "    capture.release()\n",
        "\n",
        "startVideo(args.video)\n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()\n",
        "   \n",
        "print(\"The video was successfully saved\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting yolo.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_4i5S6Hjhee"
      },
      "source": [
        "\n",
        "\n",
        "### Scripting yoloF.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjRfhgrF8nfb",
        "outputId": "0814e713-02be-4778-e95f-a762a5d7d0d0"
      },
      "source": [
        "%%writefile yoloF.py\n",
        "##### SCRIPT STARTS HERE #####\n",
        "#!usr/bin/bash python\n",
        "# Importing required packages\n",
        "#!python3 yoloF.py --video=/content/gdrive/MyDrive/yoloV3/PP.mp4\n",
        "\n",
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "import re\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Initialize the parameters\n",
        "confidenceThreshold = 0.3  #Confidence threshold\n",
        "nmsThreshold = 0.4         #Non-maximum suppression threshold\n",
        "inputWidth = 416           #Width of network's input image\n",
        "inputHeight = 416          #Height of network's input image\n",
        "\n",
        "# construct the argument parse and parse the arguments\n",
        "parser = argparse.ArgumentParser(description='Object Detection using YOLO in OPENCV')\n",
        "parser.add_argument('--image', help=\"True/False\", default=False)\n",
        "parser.add_argument('--video', help=\"Path to video file\", default=\"videos/car_on_road.mp4\")\n",
        "parser.add_argument('--verbose', help=\"To print statements\", default=True)\n",
        "args = parser.parse_args()\n",
        "\n",
        "#Load YOLO V3\n",
        "\n",
        "def loadYolo():\n",
        "\n",
        "    # load our YOLO object detector trained on COCO dataset (80 classes)\n",
        "    print(\"[INFO] loading YOLO from disk...\")\n",
        "    # derive the paths to the YOLO weights and model configuration\n",
        "    configPath = '/content/gdrive/MyDrive/yoloV3/yolov3.cfg'\n",
        "    weightsPath = '/content/gdrive/MyDrive/yoloV3/yolov3.weights'\n",
        "    net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
        "    #cv2.dnn.readNetFromDarknet(\"yolo3.weights\",\"yolov3.cfg\")\n",
        "\n",
        "    # load the COCO class labels our YOLO model was trained on\n",
        "    classes=[]\n",
        "    # load the COCO class labels our YOLO model was trained on\n",
        "    classesPath = '/content/gdrive/MyDrive/yoloV3/coco.names'\n",
        "    with open(classesPath, \"r\") as f:\n",
        "      classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    # initialize a list of colors to represent each possible class label\n",
        "    np.random.seed(42)\n",
        "    colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
        "    #np.random.randint(0, 255, size=(len(LABELS), 3),dtype=\"uint8\")\n",
        "    \n",
        "\n",
        "    # and determine only the *output* layer names that we need from YOLO\n",
        "    \n",
        "    # Get the names of all the layers in the network\n",
        "    layersNames = net.getLayerNames()\n",
        "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
        "    outputLayers = [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "    \n",
        "    return net, classes, colors, outputLayers\n",
        "\n",
        "def detectObjects(image,net,outputLayers):\n",
        "    # Create a 4D blob from a frame and then perform a forward\n",
        "    # pass of the YOLO object detector, giving us our bounding boxes\n",
        "    # and associated probabilities\n",
        "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, size=(inputWidth, inputHeight), mean=(0, 0, 0), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    layerOutputs = net.forward(outputLayers)\n",
        "    return blob, layerOutputs\n",
        "\n",
        "def getBoundingbox(layerOutputs, height, width):\n",
        "    # initialize our lists of detected bounding boxes, confidences,\n",
        "    # and class IDs, respectively\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    classIDs = []\n",
        "\n",
        "    # loop over each of the layer outputs\n",
        "    for output in layerOutputs:\n",
        "        #loop over each of the detections\n",
        "        for detection in output:\n",
        "            # extract the class ID and confidence (i.e., probability)\n",
        "            # of the current object detection\n",
        "            scores = detection[5:]\n",
        "            classID = np.argmax(scores)\n",
        "            confidence = scores[classID]\n",
        "\n",
        "            #filter out weak predictions by ensuring the detected\n",
        "            # probability is greater than the minimum probability\n",
        "\n",
        "            if confidence > confidenceThreshold:\n",
        "                # scale the bounding box coordinates back relative to\n",
        "                # the size of the image, keeping in mind that YOLO\n",
        "                # actually returns the center (x, y)-coordinates of\n",
        "                # the bounding box followed by the boxes' width and\n",
        "                # height\n",
        "                box = detection[0:4] * np.array([width, height, width, height])\n",
        "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "                \n",
        "                # use the center (x, y)-coordinates to derive the top\n",
        "                # and and left corner of the bounding box\n",
        "                x = int(centerX - (width / 2))\n",
        "                y = int(centerY - (height / 2))\n",
        "\n",
        "                # update our list of bounding box coordinates,\n",
        "                # confidences, and class IDs\n",
        "                boxes.append([x, y, int(width), int(height)])\n",
        "                confidences.append(float(confidence))\n",
        "                classIDs.append(classID)\n",
        "\n",
        "    return boxes, confidences, classIDs\n",
        "\n",
        "\n",
        "# Draw the predicted bounding boxes\n",
        "\n",
        "#def drawPredictedBB(boxes, confidences, colors, classIDs,classes, image):\n",
        "    # apply non-maxima suppression to suppress weak, overlapping\n",
        "    # bounding boxes with lower confidences\n",
        " #   indices = cv2.dnn.NMSBoxes(boxes, confidences, confidenceThreshold, nmsThreshold)\n",
        " #   font = cv2.FONT_HERSHEY_PLAIN\n",
        "    \n",
        "    # ensure at least one detection exists\n",
        " #   if len(indices) > 0:\n",
        "        #loop over indices we are keeping\n",
        " #       for index in indices.flatten():\n",
        "            # extract the bounding box coordinates\n",
        " #           (x, y) = (boxes[index][0], boxes[index][1])\n",
        " #           (width,height) = (boxes[index][2], boxes[index][3])\n",
        "\n",
        "            # Draw a bounding box\n",
        " #           text = \"{}: {:.4f}\".format(classes[classIDs[index]],confidences[index])\n",
        " #           color = [int(c) for c in colors[classIDs[index]]]\n",
        " #           cv2.rectangle(image, (x,y), (x+width, y+height), color,2)\n",
        " #           cv2.putText(image, text, (x, y - 5), font, 1, color, 1)\n",
        " #\n",
        "def videoToFrames(videoPath): \n",
        "    frameCount=0\n",
        "    inputFrames=[]\n",
        "    capture = cv2.VideoCapture(videoPath)\n",
        "    while(True):\n",
        "        # Capture the video frame by frame from the file\n",
        "        (grabbed,frame) = capture.read()\n",
        "        # if the frame was not grabbed, then we have reached the end\n",
        "        #of the stream\n",
        "        if not grabbed:\n",
        "          print(\"[INFO] All frames appended !!!\")\n",
        "          break\n",
        "        inputFrames.append(frame)\n",
        "        frameCount = frameCount + 1\n",
        "        cv2.imwrite('inputFrames/'+str(frameCount)+'.png', frame)\n",
        "    print(\"[INFO] inputFrames directory formed successfully\")\n",
        "    return inputFrames \n",
        "\n",
        "def framesToVideo():\n",
        "  outputFrames = os.listdir('output/')\n",
        "  outputFrames.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
        "  frames=[]\n",
        "  writer = None\n",
        "  (width,height)= (None, None)\n",
        "  for index in range(len(outputFrames)):\n",
        "    #reading each files\n",
        "    image = cv2.imread('output/'+outputFrames[index])\n",
        "    height, width = image.shape[:2]\n",
        "    size = (width,height)\n",
        "    \n",
        "    #inserting the frames into an image array\n",
        "    frames.append(image)\n",
        "  \n",
        "  size = (width,height)\n",
        "  outputFile = videoPath[:-4]+'_yolo_out_py.mp4'\n",
        "  # initialize our video writer\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "  writer = cv2.VideoWriter(outputFile,fourcc, 29, size)\n",
        "\n",
        "  for index in range(len(frames)):\n",
        "    # writing to a image array\n",
        "         writer.write(frames[index])\n",
        "  writer.release()\n",
        "  print(\"[INFO] Frames are stitched to video successfully\")\n",
        "\n",
        "\n",
        "def personDetection(videoPath):\n",
        "  net, classes, colors, outputLayers = loadYolo()\n",
        "  person = []\n",
        "  for i in range(len(inputFrames)):\n",
        "    frame = cv2.imread('inputFrames/'+str(i+1)+'.png')\n",
        "    height,width = frame.shape[:2]\n",
        "    # construct a blob from the input frame and then perform a forward\n",
        "\t  # pass of the YOLO object detector, giving us our bounding boxes\n",
        "\t  # and associated probabilities\n",
        "    blob, layerOutputs = detectObjects(frame, net, outputLayers)\n",
        "    boxes, confidences, classIDs = getBoundingbox(layerOutputs, height, width)\n",
        "    # apply non-maxima suppression to suppress weak, overlapping\n",
        "    # bounding boxes with lower confidences\n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidenceThreshold, nmsThreshold)\n",
        "    font = cv2.FONT_HERSHEY_PLAIN\n",
        "\n",
        "    for index in range(len(boxes)):\n",
        "      if index in indices:\n",
        "        label = str(classes[classIDs[index]])\n",
        "        if label == 'person':\n",
        "          # extract the bounding box coordinates\n",
        "          (x, y) = (boxes[index][0], boxes[index][1])\n",
        "          (w,h) = (boxes[index][2], boxes[index][3])\n",
        "            \n",
        "          # Draw a bounding box\n",
        "          text = \"{}: {:.4f}\".format(classes[classIDs[index]],confidences[index])\n",
        "          color = colors[index]\n",
        "          cv2.rectangle(frame, (x,y), (x+w, y+h), color,2)\n",
        "          cv2.putText(frame, text, (x, y - 5), font, 1, color, 1)\n",
        "          person.append([i,x, y, int(w), int(h)])  \n",
        "\n",
        "      cv2.imwrite('output/'+str(i+1)+'.png', frame)  \n",
        "\n",
        "videoPath = args.video\n",
        "inputFrames= videoToFrames(videoPath)\n",
        "person = personDetection(videoPath)\n",
        "framesToVideo()\n",
        "\n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()\n",
        "   \n",
        "print(\"The video was successfully saved\")\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting yoloF.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAHLytHnga1S"
      },
      "source": [
        "!python3 yoloF.py --video=/content/gdrive/MyDrive/yoloV3/car_chase_02.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIg7xzwzkPSL"
      },
      "source": [
        "\n",
        "\n",
        "## Explaining yoloF.py\n",
        "### Importing Necessary packages\n",
        "\n",
        "```\n",
        "import numpy as np \n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SycRGo4rk4KD"
      },
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "import re\n",
        "# from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbpdaXDjqjuz"
      },
      "source": [
        "### Pre requisites\n",
        "\n",
        "1.   Weights (yolov3.weights) : https://pjreddie.com/darknet/yolo\n",
        "2.   Configuration (yolov3.cfg): https://pjreddie.com/darknet/yolo\n",
        "3. Classes(coco.names) :  https://github.com/pjreddie/darknet/blob/master/data/coco.names\n",
        "\n",
        "The model has been trained for different sizes of images. We will download the weights and cfg files for YOLOv3–416 for now.\n",
        "\n",
        "Saved it in /content/gdrive/MyDrive/yoloV3.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw-X4_egrU2e"
      },
      "source": [
        "### Initialize the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jod0VTpskzxL"
      },
      "source": [
        "# Initialize the parameters\n",
        "confidenceThreshold = 0.3  #Confidence threshold\n",
        "nmsThreshold = 0.4         #Non-maximum suppression threshold\n",
        "inputWidth = 416           #Width of network's input image\n",
        "inputHeight = 416          #Height of network's input image"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6d-lDlZrapm"
      },
      "source": [
        "### Define functions\n",
        "\n",
        "\n",
        "1.  loadYolo\n",
        "\n",
        "\n",
        "\n",
        "*   INPUT :\n",
        "*   OUTPUT : net, classes, colors, outputLayers\n",
        "\n",
        "\n",
        "We will load **YoloV3 weights and configuration file**\n",
        "with the help of **dnn module of OpenCV**. The **coco.names** file contains the names of the different objects that our model has been trained to identify. We store them in a list called **classes**. \n",
        "\n",
        "Steps are explained along with the code.\n",
        "\n",
        "---\n",
        "\n",
        "2.   detectObjects\n",
        "\n",
        "*   INPUT : image,net,outputLayers\n",
        "*   OUTPUT : blob, layerOutputs\n",
        "\n",
        "----\n",
        "\n",
        "3.  getBoundingbox\n",
        "\n",
        "\n",
        "*   INPUT : layerOutputs, height, width\n",
        "*   OUTPUT : boxes, confidences, classIDs\n",
        "\n",
        "---\n",
        "\n",
        "4.  videoToFrames\n",
        "\n",
        "*   INPUT : path to the video file\n",
        "*   OUTPUT : input frames will be stored in inputFrames folder and we will get the array of inputFrames\n",
        "\n",
        "We can forgo the storage of frames in inputFrame file if not required.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "5.   framesToVideo\n",
        "\n",
        "\n",
        "*   INPUT : path to the video file\n",
        "*   OUTPUT : output file will be created in the directory\n",
        "\n",
        "---\n",
        "\n",
        "6.  personDetection\n",
        "\n",
        "*   INPUT : inputFrames which is the output of videoToFrames()\n",
        "*   OUTPUT : person- bounding box coordinates of the people\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFMK9qF2k0r9"
      },
      "source": [
        "#Load YOLO V3\n",
        "def loadYolo():\n",
        "\n",
        "    # load our YOLO object detector trained on COCO dataset (80 classes)\n",
        "    print(\"[INFO] loading YOLO from disk...\")\n",
        "    # derive the paths to the YOLO weights and model configuration\n",
        "    configPath = '/content/gdrive/MyDrive/yoloV3/yolov3.cfg'\n",
        "    weightsPath = '/content/gdrive/MyDrive/yoloV3/yolov3.weights'\n",
        "    net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
        "    #cv2.dnn.readNetFromDarknet(\"yolo3.weights\",\"yolov3.cfg\")\n",
        "\n",
        "    # load the COCO class labels our YOLO model was trained on\n",
        "    classes=[]\n",
        "    # load the COCO class labels our YOLO model was trained on\n",
        "    classesPath = '/content/gdrive/MyDrive/yoloV3/coco.names'\n",
        "    with open(classesPath, \"r\") as f:\n",
        "      classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    # initialize a list of colors to represent each possible class label\n",
        "    np.random.seed(42)\n",
        "    colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
        "    #np.random.randint(0, 255, size=(len(LABELS), 3),dtype=\"uint8\")\n",
        "    \n",
        "\n",
        "    # and determine only the *output* layer names that we need from YOLO\n",
        "    \n",
        "    # Get the names of all the layers in the network\n",
        "    layersNames = net.getLayerNames()\n",
        "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
        "    outputLayers = [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "    \n",
        "    return net, classes, colors, outputLayers"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CefFrdAlAn2"
      },
      "source": [
        "def detectObjects(image,net,outputLayers):\n",
        "    # Create a 4D blob from a frame and then perform a forward\n",
        "    # pass of the YOLO object detector, giving us our bounding boxes\n",
        "    # and associated probabilities\n",
        "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, size=(inputWidth, inputHeight), mean=(0, 0, 0), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    layerOutputs = net.forward(outputLayers)\n",
        "    return blob, layerOutputs"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTPhhVTOlFZy"
      },
      "source": [
        "def getBoundingbox(layerOutputs, height, width):\n",
        "    # initialize our lists of detected bounding boxes, confidences,\n",
        "    # and class IDs, respectively\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    classIDs = []\n",
        "\n",
        "    # loop over each of the layer outputs\n",
        "    for output in layerOutputs:\n",
        "        #loop over each of the detections\n",
        "        for detection in output:\n",
        "            # extract the class ID and confidence (i.e., probability)\n",
        "            # of the current object detection\n",
        "            scores = detection[5:]\n",
        "            classID = np.argmax(scores)\n",
        "            confidence = scores[classID]\n",
        "\n",
        "            #filter out weak predictions by ensuring the detected\n",
        "            # probability is greater than the minimum probability\n",
        "\n",
        "            if confidence > confidenceThreshold:\n",
        "                # scale the bounding box coordinates back relative to\n",
        "                # the size of the image, keeping in mind that YOLO\n",
        "                # actually returns the center (x, y)-coordinates of\n",
        "                # the bounding box followed by the boxes' width and\n",
        "                # height\n",
        "                box = detection[0:4] * np.array([width, height, width, height])\n",
        "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "                \n",
        "                # use the center (x, y)-coordinates to derive the top\n",
        "                # and and left corner of the bounding box\n",
        "                x = int(centerX - (width / 2))\n",
        "                y = int(centerY - (height / 2))\n",
        "#(x,y) -----------------------\n",
        "#|                            |\n",
        "#|     (centerX, centerY )    |height\n",
        "#|                            |\n",
        "#-----------------------------(x + width, y+height)\n",
        "# <------width--------------->\n",
        "                # update our list of bounding box coordinates,\n",
        "                # confidences, and class IDs\n",
        "                boxes.append([x, y, int(width), int(height)])\n",
        "                confidences.append(float(confidence))\n",
        "                classIDs.append(classID)\n",
        "\n",
        "    return boxes, confidences, classIDs"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtPjXwnUlGrh"
      },
      "source": [
        "def videoToFrames(videoPath): \n",
        "    frameCount=0\n",
        "    inputFrames=[]\n",
        "    capture = cv2.VideoCapture(videoPath)\n",
        "    while(True):\n",
        "        # Capture the video frame by frame from the file\n",
        "        (grabbed,frame) = capture.read()\n",
        "        # if the frame was not grabbed, then we have reached the end\n",
        "        #of the stream\n",
        "        if not grabbed:\n",
        "          print(\"[INFO] All frames appended !!!\")\n",
        "          break\n",
        "        inputFrames.append(frame)\n",
        "        frameCount = frameCount + 1\n",
        "        cv2.imwrite('inputFrames/'+str(frameCount)+'.png', frame)\n",
        "    print(\"[INFO] inputFrames directory formed successfully\")\n",
        "    return inputFrames"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk6lohqulJ44"
      },
      "source": [
        "def framesToVideo(videoPath):\n",
        "  outputFrames = os.listdir('output/')\n",
        "  outputFrames.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
        "  frames=[]\n",
        "  writer = None\n",
        "  (width,height)= (None, None)\n",
        "  for index in range(len(outputFrames)):\n",
        "    #reading each files\n",
        "    image = cv2.imread('output/'+outputFrames[index])\n",
        "    height, width = image.shape[:2]\n",
        "    size = (width,height)\n",
        "    \n",
        "    #inserting the frames into an image array\n",
        "    frames.append(image)\n",
        "  \n",
        "  size = (width,height)\n",
        "  outputFile = videoPath[:-4]+'_yolo_out_py.mp4'\n",
        "  # initialize our video writer\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "  writer = cv2.VideoWriter(outputFile,fourcc, 29, size)\n",
        "\n",
        "  for index in range(len(frames)):\n",
        "    # writing to a image array\n",
        "         writer.write(frames[index])\n",
        "  writer.release()\n",
        "  print(\"[INFO] Frames are stitched to video successfully\")"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IDW1ws2kON9"
      },
      "source": [
        "def personDetection(inputFrames):\n",
        "  net, classes, colors, outputLayers = loadYolo()\n",
        "  person = []\n",
        "  for i in range(len(inputFrames)):\n",
        "    frame = cv2.imread('inputFrames/'+str(i+1)+'.png')\n",
        "    height,width = frame.shape[:2]\n",
        "    # construct a blob from the input frame and then perform a forward\n",
        "\t  # pass of the YOLO object detector, giving us our bounding boxes\n",
        "\t  # and associated probabilities\n",
        "    blob, layerOutputs = detectObjects(frame, net, outputLayers)\n",
        "    boxes, confidences, classIDs = getBoundingbox(layerOutputs, height, width)\n",
        "    # apply non-maxima suppression to suppress weak, overlapping\n",
        "    # bounding boxes with lower confidences\n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidenceThreshold, nmsThreshold)\n",
        "    font = cv2.FONT_HERSHEY_PLAIN\n",
        "\n",
        "    for index in range(len(boxes)):\n",
        "      if index in indices:\n",
        "        label = str(classes[classIDs[index]])\n",
        "        if label == 'person':\n",
        "          # extract the bounding box coordinates\n",
        "          (x,y) = (boxes[index][0], boxes[index][1])\n",
        "          (w,h) = (boxes[index][2], boxes[index][3])\n",
        "            \n",
        "          # Draw a bounding box\n",
        "          text = \"{}: {:.4f}\".format(classes[classIDs[index]],confidences[index])\n",
        "          color = colors[index]\n",
        "          cv2.rectangle(frame, (x,y), (x+w, y+h), color,2)\n",
        "          cv2.putText(frame, text, (x, y - 5), font, 1, color, 1)\n",
        "          person.append([i,x, y, int(w), int(h)])  \n",
        "\n",
        "      cv2.imwrite('output/'+str(i+1)+'.png', frame)\n",
        "  return person"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixd7ZWbsljw8"
      },
      "source": [
        "def finalResult(videoPath):\n",
        "  inputFrames= videoToFrames(videoPath)\n",
        "  person = personDetection(inputFrames)\n",
        "  framesToVideo(videoPath)\n",
        "  print(\"[INFO] Image detection done successfully\")\n",
        "  return person"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69iJsJKHmOP6"
      },
      "source": [
        "### Desired result\n",
        "finalResult(\"videoPath\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhIJDg_KvHZi",
        "outputId": "f4838ba4-60d1-48d5-c25e-3886938da0c9"
      },
      "source": [
        "personBB = finalResult(\"car_chase_02.mp4\")"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] All frames appended !!!\n",
            "[INFO] inputFrames directory formed successfully\n",
            "[INFO] loading YOLO from disk...\n",
            "[INFO] Frames are stitched to video successfully\n",
            "[INFO] Image detection done successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La6ihD5MyZ3U"
      },
      "source": [
        "### The Bounding box coordinates of the persons in the video\n",
        "\n",
        "[index, x, y, width, height]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rtIr0aIx4xP"
      },
      "source": [
        "personBB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYEylS8I98R-",
        "outputId": "ef965dab-36ae-42f4-9717-f7fa27b67674"
      },
      "source": [
        "personPedestrians = finalResult(\"pedestrians.mp4\")"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] All frames appended !!!\n",
            "[INFO] inputFrames directory formed successfully\n",
            "[INFO] loading YOLO from disk...\n",
            "[INFO] Frames are stitched to video successfully\n",
            "[INFO] Image detection done successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQQ1rgws5Bqo"
      },
      "source": [
        "### Checking out whether the bounding box result is right"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lMCqTQK4_Es"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "image = cv2.imread('inputFrames/'+str(100)+'.png')\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcIdXR8G5tCo"
      },
      "source": [
        "image=cv2.rectangle(image, (100,100), (300, 300), (0,0,255), 2)\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKZiVcpOy3BG",
        "outputId": "232b6b3e-510f-43db-ff1a-914c31f03f06"
      },
      "source": [
        "personPark = finalResult(\"park.mp4\")"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] All frames appended !!!\n",
            "[INFO] inputFrames directory formed successfully\n",
            "[INFO] loading YOLO from disk...\n",
            "[INFO] Frames are stitched to video successfully\n",
            "[INFO] Image detection done successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIpJHNdR3Rnb"
      },
      "source": [
        "personPark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOUISovH0RHZ"
      },
      "source": [
        "### Distance meassurement using centroids \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQIhiUsj0hBg"
      },
      "source": [
        "We have an array of arrays with elements [index, x, y, width, height] \n",
        "where\n",
        "\n",
        "> *    index - frame index\n",
        "*   (x, y) - coordinates to derive the top and left corner of the bounding box\n",
        "*   width, height - width and height of the bounding box\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9z6jueq4kV4",
        "outputId": "2d2751bb-6364-4572-e43b-4e26aaeb1e1f"
      },
      "source": [
        "personBB[0]"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 639, 375, 67, 93]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW-iCriy4fEy"
      },
      "source": [
        "def distance()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG_W6CHD0fjw"
      },
      "source": [
        "def distanceObject(boundingBoxList){\n",
        "    \n",
        "}\n",
        "x, y, w, h = bboxes[0], bboxes[1], bboxes[2], bboxes[3]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}